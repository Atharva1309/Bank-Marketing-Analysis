{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Marketing Campaign Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description\n",
    "### Link to the dataset - https://www.kaggle.com/volodymyrgavrysh/bank-marketing-campaigns-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"bank-additional-full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age;\"job\";\"marital\";\"education\";\"default\";\"housing\";\"loan\";\"contact\";\"month\";\"day_of_week\";\"duration\";\"campaign\";\"pdays\";\"previous\";\"poutcome\";\"emp.var.rate\";\"cons.price.idx\";\"cons.conf.idx\";\"euribor3m\";\"nr.employed\";\"y\"</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56;\"housemaid\";\"married\";\"basic.4y\";\"no\";\"no\";...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57;\"services\";\"married\";\"high.school\";\"unknown...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37;\"services\";\"married\";\"high.school\";\"no\";\"ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40;\"admin.\";\"married\";\"basic.6y\";\"no\";\"no\";\"no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56;\"services\";\"married\";\"high.school\";\"no\";\"no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  age;\"job\";\"marital\";\"education\";\"default\";\"housing\";\"loan\";\"contact\";\"month\";\"day_of_week\";\"duration\";\"campaign\";\"pdays\";\"previous\";\"poutcome\";\"emp.var.rate\";\"cons.price.idx\";\"cons.conf.idx\";\"euribor3m\";\"nr.employed\";\"y\"\n",
       "0  56;\"housemaid\";\"married\";\"basic.4y\";\"no\";\"no\";...                                                                                                                                                                          \n",
       "1  57;\"services\";\"married\";\"high.school\";\"unknown...                                                                                                                                                                          \n",
       "2  37;\"services\";\"married\";\"high.school\";\"no\";\"ye...                                                                                                                                                                          \n",
       "3  40;\"admin.\";\"married\";\"basic.6y\";\"no\";\"no\";\"no...                                                                                                                                                                          \n",
       "4  56;\"services\";\"married\";\"high.school\";\"no\";\"no...                                                                                                                                                                          "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_d = pd.DataFrame(d['age;\"job\";\"marital\";\"education\";\"default\";\"housing\";\"loan\";\"contact\";\"month\";\"day_of_week\";\"duration\";\"campaign\";\"pdays\";\"previous\";\"poutcome\";\"emp.var.rate\";\"cons.price.idx\";\"cons.conf.idx\";\"euribor3m\";\"nr.employed\";\"y\"'].str.split(';',20).tolist(),columns=['age','job','marital','education','default','housing','loan','contact','month','day_of_week','duration','campaign','pdays','previous','poutcome','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>\"housemaid\"</td>\n",
       "      <td>\"married\"</td>\n",
       "      <td>\"basic.4y\"</td>\n",
       "      <td>\"no\"</td>\n",
       "      <td>\"no\"</td>\n",
       "      <td>\"no\"</td>\n",
       "      <td>\"telephone\"</td>\n",
       "      <td>\"may\"</td>\n",
       "      <td>\"mon\"</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>\"nonexistent\"</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191</td>\n",
       "      <td>\"no\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>\"services\"</td>\n",
       "      <td>\"married\"</td>\n",
       "      <td>\"high.school\"</td>\n",
       "      <td>\"unknown\"</td>\n",
       "      <td>\"no\"</td>\n",
       "      <td>\"no\"</td>\n",
       "      <td>\"telephone\"</td>\n",
       "      <td>\"may\"</td>\n",
       "      <td>\"mon\"</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>\"nonexistent\"</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191</td>\n",
       "      <td>\"no\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>\"services\"</td>\n",
       "      <td>\"married\"</td>\n",
       "      <td>\"high.school\"</td>\n",
       "      <td>\"no\"</td>\n",
       "      <td>\"yes\"</td>\n",
       "      <td>\"no\"</td>\n",
       "      <td>\"telephone\"</td>\n",
       "      <td>\"may\"</td>\n",
       "      <td>\"mon\"</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>\"nonexistent\"</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191</td>\n",
       "      <td>\"no\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>\"admin.\"</td>\n",
       "      <td>\"married\"</td>\n",
       "      <td>\"basic.6y\"</td>\n",
       "      <td>\"no\"</td>\n",
       "      <td>\"no\"</td>\n",
       "      <td>\"no\"</td>\n",
       "      <td>\"telephone\"</td>\n",
       "      <td>\"may\"</td>\n",
       "      <td>\"mon\"</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>\"nonexistent\"</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191</td>\n",
       "      <td>\"no\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>\"services\"</td>\n",
       "      <td>\"married\"</td>\n",
       "      <td>\"high.school\"</td>\n",
       "      <td>\"no\"</td>\n",
       "      <td>\"no\"</td>\n",
       "      <td>\"yes\"</td>\n",
       "      <td>\"telephone\"</td>\n",
       "      <td>\"may\"</td>\n",
       "      <td>\"mon\"</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>\"nonexistent\"</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191</td>\n",
       "      <td>\"no\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  age          job    marital      education    default housing   loan  \\\n",
       "0  56  \"housemaid\"  \"married\"     \"basic.4y\"       \"no\"    \"no\"   \"no\"   \n",
       "1  57   \"services\"  \"married\"  \"high.school\"  \"unknown\"    \"no\"   \"no\"   \n",
       "2  37   \"services\"  \"married\"  \"high.school\"       \"no\"   \"yes\"   \"no\"   \n",
       "3  40     \"admin.\"  \"married\"     \"basic.6y\"       \"no\"    \"no\"   \"no\"   \n",
       "4  56   \"services\"  \"married\"  \"high.school\"       \"no\"    \"no\"  \"yes\"   \n",
       "\n",
       "       contact  month day_of_week  ... campaign pdays previous       poutcome  \\\n",
       "0  \"telephone\"  \"may\"       \"mon\"  ...        1   999        0  \"nonexistent\"   \n",
       "1  \"telephone\"  \"may\"       \"mon\"  ...        1   999        0  \"nonexistent\"   \n",
       "2  \"telephone\"  \"may\"       \"mon\"  ...        1   999        0  \"nonexistent\"   \n",
       "3  \"telephone\"  \"may\"       \"mon\"  ...        1   999        0  \"nonexistent\"   \n",
       "4  \"telephone\"  \"may\"       \"mon\"  ...        1   999        0  \"nonexistent\"   \n",
       "\n",
       "  emp.var.rate cons.price.idx cons.conf.idx euribor3m nr.employed     y  \n",
       "0          1.1         93.994         -36.4     4.857        5191  \"no\"  \n",
       "1          1.1         93.994         -36.4     4.857        5191  \"no\"  \n",
       "2          1.1         93.994         -36.4     4.857        5191  \"no\"  \n",
       "3          1.1         93.994         -36.4     4.857        5191  \"no\"  \n",
       "4          1.1         93.994         -36.4     4.857        5191  \"no\"  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_d.to_csv('bank-additional-full1', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'bank-additional-full1.csv' does not exist: b'bank-additional-full1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7e31dca4cd0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bank-additional-full1.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'bank-additional-full1.csv' does not exist: b'bank-additional-full1.csv'"
     ]
    }
   ],
   "source": [
    "d1 = pd.read_csv(\"bank-additional-full1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating 10% missing values in the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = d1.sample(n = 12000, random_state = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = d2.job.values.tolist()\n",
    "for i in range(len(l1)):\n",
    "    \n",
    "    if l1[i] == '\"unknown\"':\n",
    "        l1[i] = np.NaN\n",
    "        \n",
    "        \n",
    "l2 = d2.marital.values.tolist()\n",
    "for i in range(len(l2)):\n",
    "    \n",
    "    if l2[i] == '\"unknown\"':\n",
    "        l2[i] = np.NaN\n",
    "\n",
    "        \n",
    "l3 = d2.education.values.tolist()\n",
    "for i in range(len(l3)):\n",
    "    \n",
    "    if l3[i] == '\"unknown\"':\n",
    "        l3[i] = np.NaN\n",
    "\n",
    "        \n",
    "l4 = d2.default.values.tolist()\n",
    "for i in range(len(l4)):\n",
    "    \n",
    "    if l4[i] == '\"unknown\"':\n",
    "        l4[i] = np.NaN\n",
    "\n",
    "        \n",
    "l5 = d2.housing.values.tolist()\n",
    "for i in range(len(l5)):\n",
    "    \n",
    "    if l5[i] == '\"unknown\"':\n",
    "        l5[i] = np.NaN\n",
    "\n",
    "        \n",
    "l6 = d2.loan.values.tolist()\n",
    "for i in range(len(l6)):\n",
    "    \n",
    "    if l6[i] == '\"unknown\"':\n",
    "        l6[i] = np.NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2a = d2.drop(d2.iloc[:, 1:6], axis = 1) \n",
    "d2a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2a['job'] = l1\n",
    "d2a['marital'] = l2\n",
    "d2a['education'] = l3\n",
    "d2a['default'] = l4\n",
    "d2a['housing'] = l5\n",
    "d2a['loan'] = l6\n",
    "\n",
    "d2a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = d2a\n",
    "d3.head()\n",
    "d3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3['emp.var.rate'] = d3['emp.var.rate'].sample(frac=.923) \n",
    "d3['cons.price.idx'] = d3['cons.price.idx'].sample(frac=.923) \n",
    "d3['cons.conf.idx'] = d3['cons.conf.idx'].sample(frac=.923) \n",
    "d3['euribor3m'] = d3['euribor3m'].sample(frac=.923) \n",
    "d3['nr.employed'] = d3['nr.employed'].sample(frac=.923) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns=d3.columns[d3.isnull().any()]\n",
    "\n",
    "[d3[null_columns].isnull().sum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3a = d3\n",
    "d3a = d3a.drop(d3a.columns[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 15, 16, 17, 18, 19, 20]], axis=1)\n",
    "d3a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the null values in the numerical column with the MEAN value of the respective column\n",
    "d3a['emp.var.rate'] = d3a['emp.var.rate'].fillna(value= d3a['emp.var.rate'].mean())\n",
    "d3a['cons.price.idx'] = d3a['cons.price.idx'].fillna(value= d3a['cons.price.idx'].mean())\n",
    "d3a['cons.conf.idx'] = d3a['cons.conf.idx'].fillna(value= d3a['cons.conf.idx'].mean())\n",
    "d3a['euribor3m'] = d3a['euribor3m'].fillna(value= d3a['euribor3m'].mean())\n",
    "d3a['nr.employed'] = d3a['nr.employed'].fillna(value= d3a['nr.employed'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3a.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = d3.drop(d3.columns[[10, 11, 12, 13, 14]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4 = pd.concat([d3, d3a], ignore_index=True, sort =False,axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4.columns = [ \"age\", \"loan\", \"contact\", \"month\", \"day_of_week\", \"duration\", \"campaign\", \"pdays\", \"previous\", \"poutcome\", \"y\", \"job\", \"marital\", \"education\", \"default\", \"housing\", \"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\", \"euribor3m\", \"nr.employed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping default column since it has 20% null values\n",
    "d4 = d4.drop(columns = 'default', axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the null values in the categorical column with the MODE of the respective column \n",
    "d4['job']= d4['job'].fillna( d4['job'].value_counts().index[0])\n",
    "d4['marital']=d4['marital'].fillna(d4['marital'].value_counts().index[0])\n",
    "d4['education']=d4['education'].fillna(d4['education'].value_counts().index[0])\n",
    "d4['housing']=d4['housing'].fillna(d4['housing'].value_counts().index[0])\n",
    "d4['loan']=d4['loan'].fillna(d4['loan'].value_counts().index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a seperate dataframe for categorical variables\n",
    "X = d4[['job', 'marital', 'education', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']]\n",
    "d5 = d4.drop(['job', 'marital', 'education', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome'],axis=1)\n",
    "X.head(n=5)\n",
    "d5.head()\n",
    "\n",
    "#Creating the dummy variable for categorical columns\n",
    "X= pd.get_dummies(X)\n",
    "X.head()\n",
    "d6 = pd.concat([X,d5],axis=1)\n",
    "d6.head()\n",
    "\n",
    "#Resetting the index values of the dataframe starting from 0 \n",
    "d6.reset_index(inplace = True)\n",
    "d6.head()\n",
    "\n",
    "#Mapping the target column where 'yes' = 1 and 'no' = 0\n",
    "d11 = d6.y.astype(\"category\").cat.codes\n",
    "d12 = pd.DataFrame(d11, columns= ['target'])\n",
    "d12.head()\n",
    "\n",
    "#Appending it to the original dataframe\n",
    "d_final = pd.concat([d6,d12],axis=1)\n",
    "d_final.head()\n",
    "\n",
    "#Dropping the index column and y column which has the unmapped values \n",
    "d_final2 = d_final.drop(['index','y'], axis = 1)\n",
    "d_final2.head()\n",
    "\n",
    "df2 = d_final2\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for data correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_cor = df2.corr()\n",
    "\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(30,30)) \n",
    "sns.heatmap(cl_cor,annot=True,cmap='RdBu_r',linewidths=0.30, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis for categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting barplots for job, marital, education, contact, loan, housing, day of week, poutcome vs target count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [d4.job, d4.marital, d4.education, d4.contact, d4.loan, d4.housing, d4.day_of_week, d4.poutcome]\n",
    "for p in range(len(lst)):\n",
    "    sns.set(rc={'figure.figsize':(10,10)})\n",
    "    sns.set_context(\"talk\", font_scale=0.5)\n",
    "    gen = sns.countplot(x= lst[p], hue='y', data=d4 )\n",
    "    gen.set_xticklabels(lst[p].unique())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis for numerical columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d7 = pd.DataFrame(df2, columns = ['duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cor = d7.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(25,5)) \n",
    "sns.heatmap(num_cor,annot=True,cmap='RdBu_r',linewidths=0.30, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the two columns euribor3m and nr.employed since they are not much related with the target column and are also\n",
    "# highly correlated which might affect our classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop(['nr.employed','euribor3m'],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating numpy arrays for features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.drop('target',axis=1).values\n",
    "y = df2['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.2)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. KNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "train_score_array = []\n",
    "test_score_array = []\n",
    "\n",
    "for k in range(1,20):\n",
    "    knn = KNeighborsClassifier(k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_score_array.append(knn.score(X_train, y_train))\n",
    "    test_score_array.append(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x_axis = range(1,20)\n",
    "%matplotlib inline\n",
    "plt.plot(x_axis, train_score_array, label = 'Train Score')\n",
    "plt.plot(x_axis, test_score_array, label = 'Test Score')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(10)\n",
    "knn.fit(X_train, y_train)\n",
    "print('Train score: {:.4f}'.format(knn.score(X_train, y_train)))\n",
    "print('Test score: {:.4f}'.format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_parms_knn = {'n_neighbors':[1,5,10,15,20,25]}\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), grid_parms_knn,verbose = 1,cv = 3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "kfold = KFold(n_splits=10)\n",
    "print(\"Cross-validation scores:\\n{}\".format(cross_val_score(knn , X_train, y_train, cv=kfold)))\n",
    "scores = cross_val_score(knn , X_train, y_train, cv=kfold)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    " \n",
    "knn = KNeighborsClassifier(20)\n",
    "knn.fit(X_train, y_train)\n",
    "ypred = knn.predict(X_test)\n",
    "cm = confusion_matrix(ypred,y_test)\n",
    "f = f1_score(ypred,y_test)\n",
    "print(cm)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classification summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train score : 0.8982\n",
    "### Test score : 0.8950\n",
    "### Cross validation score : 0.89260\n",
    "### Best Parameters : {'n_neighbors': 10}\n",
    "### F1 score : 0.18060"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.2)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "c_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "train_score_l1 = []\n",
    "train_score_l2 = []\n",
    "test_score_l1 = []\n",
    "test_score_l2 = []\n",
    "\n",
    "for c in c_range:\n",
    "    log_l1 = LogisticRegression(penalty = 'l1', C = c)\n",
    "    log_l2 = LogisticRegression(penalty = 'l2', C = c)\n",
    "    log_l1.fit(X_train, y_train)\n",
    "    log_l2.fit(X_train, y_train)\n",
    "    train_score_l1.append(log_l1.score(X_train, y_train))\n",
    "    train_score_l2.append(log_l2.score(X_train, y_train))\n",
    "    test_score_l1.append(log_l1.score(X_test, y_test))\n",
    "    test_score_l2.append(log_l2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(c_range, train_score_l1, label = 'Train score, penalty = l1')\n",
    "plt.plot(c_range, test_score_l1, label = 'Test score, penalty = l1')\n",
    "plt.plot(c_range, train_score_l2, label = 'Train score, penalty = l2')\n",
    "plt.plot(c_range, test_score_l2, label = 'Test score, penalty = l2')\n",
    "plt.legend()\n",
    "plt.xlabel('Regularization parameter: C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_l2 = LogisticRegression(penalty = 'l2', C = 1)\n",
    "log_l2.fit(X_train, y_train)\n",
    "print(log_l2.score(X_train, y_train))\n",
    "print(log_l2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_l1 = LogisticRegression(penalty = 'l1', C = 1)\n",
    "log_l1.fit(X_train, y_train)\n",
    "print(log_l1.score(X_train, y_train))\n",
    "print(log_l1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = LogisticRegression()\n",
    "penalty = ['l1', 'l2']\n",
    "C = [0.001,0.01,0.1,1,10,100]\n",
    "parameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "grid_search = GridSearchCV(model,parameters, cv=6, return_train_score=True)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, random_state=0)\n",
    "grid_search.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "log_l1 = LogisticRegression(penalty = 'l2', C = 100)\n",
    "kfold = KFold(n_splits=6)\n",
    "print(\"Cross-validation scores:\\n{}\".format(cross_val_score(log_l1 , X_train, y_train, cv=kfold)))\n",
    "scores = cross_val_score(log_l1 , X_train, y_train, cv=kfold)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "log_l1 = LogisticRegression(penalty = 'l2', C = 100)\n",
    "log_l1.fit(X_train, y_train)\n",
    "ypred = log_l1.predict(X_test)\n",
    "ft = f1_score(ypred,y_test)\n",
    "cm = confusion_matrix(ypred,y_test)\n",
    "print(ft)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train score : 0.91\n",
    "### Test score : 0.91083\n",
    "### Cross validation score : 0.90677\n",
    "### Best Parameters : {'C': 100, 'penalty': 'l2'} \n",
    "### F1 score : 0.50678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Kernel Support Vector Machine  - Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.2)\n",
    "X_train\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "index = []\n",
    "\n",
    "for C in [0.001, 0.01,0.1, 1, 10, 100]:\n",
    "        #create the model\n",
    "        svc = SVC(kernel = 'linear', C = C)\n",
    "        \n",
    "        #train the model\n",
    "        svc.fit(X_train, y_train)\n",
    "        \n",
    "        #evaluate the model\n",
    "        train.append(svc.score(X_train, y_train))\n",
    "        test.append(svc.score(X_test, y_test))\n",
    "        index.append((C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train)\n",
    "print(test)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = SVC(kernel = 'linear')\n",
    "\n",
    "parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid_search = GridSearchCV(model,parameters, cv=3, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "svc = SVC(kernel = 'linear',C = 10)\n",
    "kfold = KFold(n_splits=6)\n",
    "#print(\"Cross-validation scores:\\n{}\".format(cross_val_score(svc , X_train, y_train, cv=kfold)))\n",
    "scores = cross_val_score(svc , X_train, y_train, cv=kfold)\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "svc = SVC(kernel = 'linear', C = 10 )\n",
    "svc.fit(X_train, y_train)\n",
    "ypred = svc.predict(X_test)\n",
    "cm = confusion_matrix(ypred,y_test)\n",
    "f = f1_score(ypred,y_test)\n",
    "print(cm)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Support Vector Machine  - Linear Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train score : 0.90375\n",
    "### Test score : 0.908333\n",
    "### Cross validation score : 0.90281\n",
    "### Best Parameters : {'C': 10}\n",
    "### F1 score : 0.45544"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Kernelized Support Vector Machine  - rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "index = []\n",
    "\n",
    "for C in [0.001, 0.01,0.1, 1, 10, 100]:\n",
    "    for gamma in [0.001, 0.01,0.1, 1, 10, 100]:\n",
    "        #create the model\n",
    "        svc = SVC(kernel = 'rbf', C = C, gamma = gamma)\n",
    "        \n",
    "        #train the model\n",
    "        svc.fit(X_train, y_train)\n",
    "        \n",
    "        #evaluate the model\n",
    "        train.append(svc.score(X_train, y_train))\n",
    "        test.append(svc.score(X_test, y_test))\n",
    "        index.append((C,gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train)\n",
    "print(test)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = SVC(kernel = 'rbf')\n",
    "\n",
    "parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma' : [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid_search = GridSearchCV(model,parameters, cv=6, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "svc1 = SVC(kernel = 'rbf',C = 100,gamma = 0.01)\n",
    "kfold = KFold(n_splits=6)\n",
    "#print(\"Cross-validation scores:\\n{}\".format(cross_val_score(svc , X_train, y_train, cv=kfold)))\n",
    "scores = cross_val_score(svc1 , X_train, y_train, cv=kfold)\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "svc = SVC(kernel = 'rbf', C = 100 )\n",
    "svc.fit(X_train, y_train)\n",
    "ypred = svc.predict(X_test)\n",
    "cm = confusion_matrix(ypred,y_test)\n",
    "fl = f1_score(ypred,y_test)\n",
    "print(cm)\n",
    "print(fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Support Vector Machine  - rbf Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train score : 0.91688\n",
    "### Test score : 0.90633\n",
    "### Cross validation score : 0.90244\n",
    "### Best Parameters : {'C': 100, 'gamma': 0.001}\n",
    "### F1 score : 0.46777"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5. Kernelized Support Vector Machine  - poly\n",
    "### Poly kernel takes a lot of time to compute and is out of the computational power of my laptop, hence I have only provided the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "index = []\n",
    "\n",
    "for gamma in [0.001, 0.01,0.1, 1, 10, 100]:\n",
    "    #create the model\n",
    "    svc = SVC(kernel = 'poly', degree = 2, gamma = gamma)\n",
    "    \n",
    "    #train the model\n",
    "    svc.fit(X_train, y_train)\n",
    "    \n",
    "    #evaluate the model\n",
    "    train.append(svc.score(X_train, y_train))\n",
    "    test.append(svc.score(X_test, y_test))\n",
    "    index.append((gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train)\n",
    "print(test)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  6. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "dtree.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(dtree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(dtree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(max_depth=5, random_state=0)\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(dtree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(dtree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_b =  X_train[:2000,[2,9]]\n",
    "y_b = y_train[:2000]\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_b, y_b)\n",
    "\n",
    "plot_decision_regions(X_b, y_b, clf = dtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "dtree = DecisionTreeClassifier(max_depth=11, random_state=0)\n",
    "kfold = KFold(n_splits=6)\n",
    "print(\"Cross-validation scores:\\n{}\".format(cross_val_score(dtree , X_train, y_train, cv=kfold)))\n",
    "scores = cross_val_score(dtree , X_train, y_train, cv=kfold)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "dtree = DecisionTreeClassifier(max_depth=11, random_state=0)\n",
    "dtree.fit(X_train, y_train)\n",
    "ypred = dtree.predict(X_test)\n",
    "cm = confusion_matrix(ypred,y_test)\n",
    "ft = f1_score(ypred,y_test)\n",
    "print(cm)\n",
    "print(ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train score : 0.914\n",
    "### Test score : 0.901\n",
    "### Cross validation score : 0.89011\n",
    "### F1 score : 0.44247"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  7. Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "train = []\n",
    "test = []\n",
    "index = []\n",
    "\n",
    "for C in [0.001, 0.01,0.1, 1, 10, 100]:\n",
    "        #create the model\n",
    "        svc = LinearSVC(C = C)\n",
    "        \n",
    "        #train the model\n",
    "        svc.fit(X_train, y_train)\n",
    "        \n",
    "        #evaluate the model\n",
    "        train.append(svc.score(X_train, y_train))\n",
    "        test.append(svc.score(X_test, y_test))\n",
    "        index.append((C))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train)\n",
    "print(test)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = LinearSVC(C = C)\n",
    "\n",
    "parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid_search = GridSearchCV(model,parameters, cv=6, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "svc1 = LinearSVC(C = 1)\n",
    "kfold = KFold(n_splits=6)\n",
    "scores = cross_val_score(svc1 , X_train, y_train, cv=kfold)\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "svc = LinearSVC(C = 1 )\n",
    "svc.fit(X_train, y_train)\n",
    "ypred = svc.predict(X_test)\n",
    "cm = confusion_matrix(ypred,y_test)\n",
    "fl = f1_score(ypred,y_test)\n",
    "print(cm)\n",
    "print(fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train score : 0.90666\n",
    "### Test score : 0.90933\n",
    "### Cross validation score : 0.91\n",
    "### Best Parameters : {'C': 1}\n",
    "### F1 score : 0.4645"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the F-1 scores of all the models, I conclude that the Logistic Regression has the highest F-1 score which is supported by the confusion matrix. Hence Logistic Regression is good model for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
